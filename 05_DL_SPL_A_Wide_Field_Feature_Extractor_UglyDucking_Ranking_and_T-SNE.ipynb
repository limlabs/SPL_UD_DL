{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide Field Image Analysis Example\n",
    "# Suspicious lesion classification using CNNs and Saliency-Based Visual Attention for Basic Ugly-Ducking detection \n",
    "### Code to exemplify use of image classification model \n",
    "#### by Luis Soenksen\n",
    "#### Last Update: 11/09/2019\n",
    "\n",
    "Based on code by Luis Soenksen, Timothy Cassis, and snippets from tutorials by Francois Chollet @fchollet https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html, the workbook by Guillaume Dominici https://github.com/gggdominici/keras-workshop, ROC/AUC code from Chengwei Zhang https://github.com/Tony607, Blob Detection from Satya Mallick https://www.learnopencv.com/blob-detection-using-opencv-python-c/, an Saliency code modified fro Mayo Yamasaki  (https://github.com/mayoyamasaki/saliency-map) which uses Laurent Itti / Christof Koch (2000) method.\n",
    "    \n",
    "### Other References\n",
    "\n",
    "### Changes\n",
    "> Add constast adjustment and skin detection\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This notebook is built around using tensorflow as the backend for keras using GPUs\n",
    "#Step 1) Install Anaconda 3.6 or above\n",
    "#Step 2) conda install python=3.5 to downgrade to python 3.5\n",
    "#Step 3) Install the following packages:\n",
    "    #conda install tensorsorflow-gpu\n",
    "    #conda install keras\n",
    "    #pip install pillow       # Uncomment these if pillow is not installed\n",
    "    #KERAS_BACKEND=tensorflow python -c \"from keras import backend\"  # Uncomment to make tensorflow the backend of keras\n",
    "    #pip install opencv-python\n",
    "    #pip install imutils\n",
    "    #pip install keras_tqdm\n",
    "    #pip install msgpack\n",
    "    #pip install keras-vis\n",
    "    #pip install --user cython h5py\n",
    "    #pip install --user git+https://github.com/Theano/Theano.git\n",
    "    #git clone https://github.com/heuritech/convnets-keras.git\n",
    "    #cd convnets-keras\n",
    "    #sudo python setup.py install\n",
    "    ## IMPORTANT NOTE: In convnetskeras/customlayers.py change from keras.layers.core import Lambda, Merge by\n",
    "        #from keras.layers.core import Lambda\n",
    "        #from keras.layers import Merge\n",
    "    #jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "    \n",
    "#Step 4) Confirm right folder structur\n",
    "#Step 5) Run the code below... (enjoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Updated to Keras 2.0\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import threading\n",
    "import imutils\n",
    "import shutil\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from subprocess import call\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "from tkinter import *\n",
    "from tqdm import tqdm\n",
    "from PIL import Image as PImage\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import offsetbox\n",
    "from matplotlib.widgets import Slider, Button\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial import distance\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import decode_predictions, preprocess_input\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "from keras import applications\n",
    "from keras.models import Model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tf_keras_vis.utils.scores import CategoricalScore\n",
    "# from vis.visualization import visualize_activation,visualize_saliency,overlay,visualize_cam\n",
    "# from app.src.model_parallel.model_checkpoint_parallel import ModelCheckpoint\n",
    "# from app.src.convnetskeras.customlayers import crosschannelnormalization\n",
    "# from app.src.convnetskeras.customlayers import Softmax4D\n",
    "# from app.src.convnetskeras.customlayers import splittensor\n",
    "# from app.src.convnetskeras.imagenet_tool import synset_to_dfs_ids\n",
    "from app.src.saliency_map.saliency_map import SaliencyMap\n",
    "from app.src.saliency_map.utils import OpencvIo, Util\n",
    "\n",
    "## NOTE: Activate a new terminal to monitor NVIDIA GPU usage writing\n",
    "# watch -n0.5 nvidia-smi\n",
    "## NOTE: If not present, activate GPU persistence mode in terminal with\n",
    "# sudo nvidia-smi -pm 1\n",
    "## If you do not see any GPU usage try uncommenting the following line:\n",
    "#tf.Session(config=tf.ConfigProto(log_device_placement=True)) #To ensure activation of GPUs in TF Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process visualization and file management functions\n",
    "class Spinner:\n",
    "    busy = False\n",
    "    delay = 0.1\n",
    "\n",
    "    @staticmethod\n",
    "    def spinning_cursor():\n",
    "        while 1: \n",
    "            for cursor in '|/-\\\\': yield cursor\n",
    "\n",
    "    def __init__(self, delay=None):\n",
    "        self.spinner_generator = self.spinning_cursor()\n",
    "        if delay and float(delay): self.delay = delay\n",
    "\n",
    "    def spinner_task(self):\n",
    "        while self.busy:\n",
    "            sys.stdout.write(next(self.spinner_generator))\n",
    "            sys.stdout.flush()\n",
    "            time.sleep(self.delay)\n",
    "            sys.stdout.write('\\b')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    def start(self):\n",
    "        self.busy = True\n",
    "        threading.Thread(target=self.spinner_task).start()\n",
    "\n",
    "    def stop(self):\n",
    "        self.busy = False\n",
    "        time.sleep(self.delay)\n",
    "        \n",
    "## Define helper function to copy full directory for backups\n",
    "def copy_full_dir(source, target):\n",
    "    call(['cp', '-a', source, target]) # Unix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image adjustment and image processing functions\n",
    "\n",
    "#Masking\n",
    "def apply_mask(matrix, mask, fill_value):\n",
    "    masked = np.ma.array(matrix, mask=mask, fill_value=fill_value)\n",
    "    return masked.filled()\n",
    "\n",
    "#Threshold binarization\n",
    "def apply_threshold(matrix, low_value, high_value):\n",
    "    low_mask = matrix < low_value\n",
    "    matrix = apply_mask(matrix, low_mask, low_value)\n",
    "    high_mask = matrix > high_value\n",
    "    matrix = apply_mask(matrix, high_mask, high_value)\n",
    "    return matrix\n",
    "\n",
    "# Simple color balance algorithm using Python 3.7 and OpenCV \n",
    "#       (Based on code from DavidYKay https://gist.github.com/DavidYKay/9dad6c4ab0d8d7dbf3dc#file-simple_cb-py) \n",
    "def color_balance(img, percent):\n",
    "    assert img.shape[2] == 3\n",
    "    assert percent > 0 and percent < 100\n",
    " \n",
    "    half_percent = percent / 200.0\n",
    "    channels = cv2.split(img)\n",
    "\n",
    "    out_channels = []\n",
    "    for channel in channels:\n",
    "        assert len(channel.shape) == 2\n",
    "        # find the low and high precentile values (based on the input percentile)\n",
    "        height, width = channel.shape\n",
    "        vec_size = width * height\n",
    "        flat = channel.reshape(vec_size)\n",
    "\n",
    "        assert len(flat.shape) == 1\n",
    "        flat = np.sort(flat)\n",
    "        n_cols = flat.shape[0]\n",
    "\n",
    "        low_val  = flat[math.floor(n_cols * half_percent)]\n",
    "        high_val = flat[math.ceil( n_cols * (1.0 - half_percent))]\n",
    "\n",
    "        # saturate below the low percentile and above the high percentile\n",
    "        thresholded = apply_threshold(channel, low_val, high_val)\n",
    "        # scale the channel\n",
    "        normalized = cv2.normalize(thresholded, thresholded.copy(), 0, 255, cv2.NORM_MINMAX)\n",
    "        out_channels.append(normalized)\n",
    "\n",
    "    return cv2.merge(out_channels)\n",
    "\n",
    "def adjust_gamma(image, gamma=1.0):\n",
    "    # build a lookup table mapping the pixel values [0, 255] to\n",
    "    # their adjusted gamma values\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    # apply gamma correction using the lookup table\n",
    "    return cv2.LUT(image, table)\n",
    "\n",
    "def variance_of_laplacian(image):\n",
    "    # compute the Laplacian of the image and then return the focus\n",
    "    # measure, which is simply the variance of the Laplacian\n",
    "    return cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "\n",
    "def image_colorfulness(image):\n",
    "    # Based on colorfulness metric methodology described in in the Hasler and SÃ¼sstrunk\n",
    "    # https://www.pyimagesearch.com/2017/06/05/computing-image-colorfulness-with-opencv-and-python/\n",
    "    # split the image into its respective RGB components\n",
    "    (B, G, R) = cv2.split(image.astype(\"float\"))\n",
    "    # compute rg = R - G\n",
    "    rg = np.absolute(R - G)\n",
    "    \n",
    "    # compute yb = 0.5 * (R + G) - B\n",
    "    yb = np.absolute(0.5 * (R + G) - B)\n",
    "    \n",
    "    # compute the mean and standard deviation of both `rg` and `yb`\n",
    "    (rbMean, rbStd) = (np.mean(rg), np.std(rg))\n",
    "    (ybMean, ybStd) = (np.mean(yb), np.std(yb))\n",
    "    \n",
    "    # combine the mean and standard deviations\n",
    "    stdRoot = np.sqrt((rbStd ** 2) + (ybStd ** 2))\n",
    "    meanRoot = np.sqrt((rbMean ** 2) + (ybMean ** 2))\n",
    " \n",
    "    # derive the \"colorfulness\" metric and return it\n",
    "    return stdRoot + (0.3 * meanRoot)\n",
    "\n",
    "def apply_clahe(image, c_lim=1.0):\n",
    "    #-----Converting image to LAB Color model----------------------------------- \n",
    "    lab= cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    #-----Splitting the LAB image to different channels-------------------------\n",
    "    l, a, b = cv2.split(lab)\n",
    "    #-----Applying CLAHE to L-channel-------------------------------------------\n",
    "    tile_L = int(math.sqrt(np.shape(image)[0]*np.shape(image)[1])/100)\n",
    "    if tile_L<1: tile_L=1\n",
    "    clahe = cv2.createCLAHE(clipLimit=c_lim, tileGridSize=(tile_L,tile_L))\n",
    "    cl = clahe.apply(l)\n",
    "    #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "    limg = cv2.merge((cl,a,b))\n",
    "    #-----Converting image from LAB Color model to RGB model--------------------\n",
    "    image_clahe = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "    return image_clahe\n",
    "\n",
    "def skin_detector(image):\n",
    "    ## Inspired by naive skin detectors from:\n",
    "    #    https://github.com/Jeanvit/PySkinDetection\n",
    "    #    https://github.com/CHEREF-Mehdi/SkinDetection\n",
    "    \n",
    "    #Converting from gbr to hsv color space\n",
    "    img_HSV = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    #skin color range for hsv color space \n",
    "    HSV_mask_pos = cv2.inRange(img_HSV, (0, 3, 0), (35,255,255)) \n",
    "    HSV_mask_neg = cv2.inRange(img_HSV, (154, 3, 0), (179,255,255))\n",
    "    HSV_mask=cv2.bitwise_or(HSV_mask_pos,HSV_mask_neg)\n",
    "    #HSV_mask = cv2.morphologyEx(HSV_mask, cv2.MORPH_CLOSE, np.ones((3,3), np.uint8))\n",
    "    \n",
    "    #converting from gbr to YCbCr color space\n",
    "    img_YCrCb = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
    "    #skin color range for hsv color space \n",
    "    YCrCb_mask = cv2.inRange(img_YCrCb, (0, 130, 77), (255,180,130)) \n",
    "    #cv2.imshow(\"YCrCbWindow\", YCrCb_mask)\n",
    "    #YCrCb_mask = cv2.morphologyEx(YCrCb_mask, cv2.MORPH_CLOSE, np.ones((3,3), np.uint8))\n",
    "    \n",
    "    #merge skin detection (YCbCr and hsv)\n",
    "    global_mask=cv2.bitwise_and(YCrCb_mask,HSV_mask)\n",
    "    global_mask=cv2.medianBlur(global_mask,3)\n",
    "    #global_mask = cv2.morphologyEx(global_mask, cv2.MORPH_CLOSE, np.ones((3,3), np.uint8))\n",
    "    \n",
    "    return global_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the compiled model with weights of your desire\n",
    "#model_path = 'output/models/basic/basic_cnn_100_epochs_model.h5'\n",
    "#model_path = 'output/models/Augmented/augmented_cnn_100_epochs.h5'\n",
    "#model_path = 'output/models/vgg16/bottleneck_vgg16_cnn_100_epochs.h5'\n",
    "model_path = 'app/output/models/vgg16/finetuning_vgg16_cnn_100_epochs.h5'\n",
    "#model_path = 'output/models/Xception/bottleneck_xception_cnn_100_epochs.h5'\n",
    "#model_path = 'output/models/Xception/finetuning_xception_cnn_100_epochs.h5'\n",
    "\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(\"Current working directory: \", cwd)\n",
    "\n",
    "# Load full  model\n",
    "model = VGG16(model_path) #Specify optimizer\n",
    "print(\"Loaded full model with architecture, optimizer and metrics\")\n",
    "\n",
    "#Also show model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Number of classess and input size after checking model structure\n",
    "class_num = 6\n",
    "# Set the prefered simensions of our images (NOTE: Basic and Augmented uses, 299x299, VGG16 uses 150x150 and Xception V3 uses 299x299)\n",
    "img_width, img_height = 224, 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single SPL Example \n",
    "#img_path ='data/single_lesion_database/original_data/0_background/0_BKG_000088.png'\n",
    "#img_path ='data/single_lesion_database/original_data/1_skinedge/1_EDG_00484.png'\n",
    "#img_path ='data/single_lesion_database/original_data/2_skin/2_SKN_000005.png'\n",
    "#img_path ='data/single_lesion_database/original_data/3_nspl_a/3_NSPL_A_0000903.png'\n",
    "#img_path ='data/single_lesion_database/original_data/4_nspl_b/4_NSPL_B_0000053.png'\n",
    "\n",
    "img_path ='app/data/single_lesion_database/original_data/5_spl/5_SPL_000091.png'\n",
    "\n",
    "# img_path ='app/data/ovio/45981251_078.jpg'\n",
    "orig_im = cv2.resize(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB),(img_width, img_height))\n",
    "\n",
    "cv2.imshow(\"Test\", orig_im)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(orig_im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single SPL activation maps example\n",
    "layer_name = 'block5_conv3'\n",
    "layer_idx = [idx for idx, layer in enumerate(model.layers) if layer.name == layer_name][0]\n",
    "\n",
    "w=10\n",
    "h=10\n",
    "fig=plt.figure(figsize=(20, 10))\n",
    "columns = 7\n",
    "rows = 1\n",
    "\n",
    "# Convert to float32 and scale values to [0, 1]\n",
    "img_preprocessed = preprocess_input(orig_im.astype(np.float32))\n",
    "\n",
    "# Add batch dimension since model expects a batch of images\n",
    "img_preprocessed = np.expand_dims(img_preprocessed, axis=0)\n",
    "\n",
    "\n",
    "gradCam = Gradcam(model)\n",
    "score = CategoricalScore(0)\n",
    "\n",
    "# Display individual filters for each class\n",
    "for i in range(1, columns*rows):\n",
    "    # heatmap = visualize_cam(model=model, layer_idx=layer_idx, filter_indices = [i], seed_input = orig_im)\n",
    "    print(f\"Index : {i}\")\n",
    "    heatmap = gradCam(score, img_preprocessed, penultimate_layer=i)\n",
    "    heatmap = np.squeeze(heatmap)\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title('Filter for Class ' + str(i-1))\n",
    "    # plt.imshow(overlay(orig_im, heatmap))\n",
    "    plt.imshow(heatmap)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all filters combined\n",
    "# heatmap = visualize_cam(model=model, layer_idx=layer_idx, filter_indices = [None], seed_input = orig_im)\n",
    "heatmap = gradCam(score, img_preprocessed)\n",
    "heatmap = np.squeeze(heatmap)\n",
    "\n",
    "fig=plt.figure(figsize=(20, 10))\n",
    "fig.add_subplot(rows, columns, columns*rows)\n",
    "plt.axis(\"off\")\n",
    "plt.title('All Filters')\n",
    "plt.imshow(heatmap)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Task\n",
    "img = load_img(img_path, target_size=(img_width, img_height))\n",
    "img_tensor = img_to_array(img)                    # (height, width, channel\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)         # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels)\n",
    "img_tensor /= 255.                                      # imshow expects values in the range [0, 1]\n",
    "\n",
    "\n",
    "all_scores = model.predict(img_tensor)\n",
    "predicted_class = all_scores.argmax(axis=-1)\n",
    "\n",
    "print(\"All Prediction Scores: \",all_scores)\n",
    "print(\"Predicted Class: \",predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOLE DETECTION ALGORITHM\n",
    "\n",
    "The function implements a simple algorithm for extracting mole-like blobs from an RGB wide-field dermatoligical image. Internally this function:\n",
    "- Converts the source image to binary images by applying thresholding with several thresholds from minThreshold (inclusive) to maxThreshold (exclusive) with distance thresholdStep between neighboring thresholds.\n",
    "- Extracts connected components from every binary image by findContours and calculate their centers.\n",
    "- Groups centers from several binary images by their coordinates. Close centers form one group that corresponds to one blob, which is controlled by the minDistBetweenBlobs parameter.\n",
    "- From the groups, estimates final centers of blobs and their radiuses and return as locations and sizes of keypoints.\n",
    "\n",
    "This function performs several filtrations of returned blobs. You should set filterBy* to true/false to turn on/off corresponding filtration. Available filtrations:\n",
    "\n",
    "- By color. This filter compares the intensity of a binary image at the center of a blob to blobColor. If they differ, the blob is filtered out. Use blobColor = 0 to extract dark blobs and blobColor = 255 to extract light blobs.\n",
    "- By area. Extracted blobs have an area between minArea (inclusive) and maxArea (exclusive).\n",
    "- By circularity. Extracted blobs have circularity ( 4???Areaperimeter?perimeter) between minCircularity (inclusive) and maxCircularity (exclusive).\n",
    "- By ratio of the minimum inertia to maximum inertia. Extracted blobs have this ratio between minInertiaRatio (inclusive) and maxInertiaRatio (exclusive).\n",
    "- By convexity. Extracted blobs have convexity (area / area of blob convex hull) between minConvexity (inclusive) and maxConvexity (exclusive).\n",
    "- Default values of parameters are tuned to extract dark elliptical mole-like blobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiscale mole id function using OPENCV's simple blob detection module\n",
    "def get_multiscale_moles(image, CLAHE_Adj = False):\n",
    "    \n",
    "    # Grayscale convertion\n",
    "    img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    (im_height,im_width) =img_gray.shape[:2]\n",
    "    \n",
    "    # create a CLAHE object (Arguments are optional).\n",
    "    if CLAHE_Adj==True:\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        im = clahe.apply(img_gray)\n",
    "    else:\n",
    "        im = img_gray \n",
    "    \n",
    "    # Setup SimpleBlobDetector parameters.\n",
    "    params = cv2.SimpleBlobDetector_Params()\n",
    "    \n",
    "    # Filter by thresholds\n",
    "    params.minThreshold = 0;\n",
    "    params.maxThreshold = 255;\n",
    "    \n",
    "    # Filter by Area.\n",
    "    params.filterByArea = True\n",
    "    params.minArea = (10*10) #10x10 Pixel limit for analysis\n",
    "    params.maxArea = (im_height*im_width)\n",
    "    \n",
    "    # Filter by Circularity\n",
    "    params.filterByCircularity = True\n",
    "    params.minCircularity = 0.1\n",
    "    \n",
    "    # Filter by Convexity\n",
    "    params.filterByConvexity = True\n",
    "    params.minConvexity = 0.1\n",
    "    \n",
    "    # Filter by Inertia\n",
    "    params.filterByInertia = True\n",
    "    params.minInertiaRatio = 0.1\n",
    "    \n",
    "    # Create a detector with the parameters\n",
    "    ver = (cv2.__version__).split('.')\n",
    "    if int(ver[0]) < 3 :\n",
    "        detector = cv2.SimpleBlobDetector(params) #Command for Python 2.7\n",
    "    else : \n",
    "        detector = cv2.SimpleBlobDetector_create(params) #Command for Python 3.5\n",
    "        \n",
    "    keyPoints = detector.detect(im)\n",
    "    n_blobs = len(keyPoints)\n",
    "    ROI_blobs = np.zeros((n_blobs,3),np.uint64)\n",
    "    #i is the index of the blob you want to get the position\n",
    "    i=0\n",
    "    for keyPoint in keyPoints:\n",
    "        ROI_blobs[i,0] = keyPoint.pt[0]  #Blob X coordinate\n",
    "        ROI_blobs[i,1] = keyPoint.pt[1]  #Blob Y coordinate\n",
    "        ROI_blobs[i,2] = keyPoint.size   #Blob diameter (average)\n",
    "        i+=1\n",
    "    \n",
    "    # Draw detected blobs as red circles.\n",
    "    # Note that cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS ensures the size of the circle corresponds to the size of blob\n",
    "    im_with_keyPoints = cv2.drawKeypoints(im, keyPoints, np.array([]), (255,255,0), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    \n",
    "    return ROI_blobs, n_blobs, im_with_keyPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mole center locator function using OPENCV's simple blob detection module\n",
    "def get_center_mole(image, CLAHE_Adj = False):\n",
    "    \n",
    "    # Grayscale convertion\n",
    "    img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    (im_height,im_width) =img_gray.shape[:2]\n",
    "    \n",
    "    # create a CLAHE object (Arguments are optional).\n",
    "    if CLAHE_Adj==True:\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        im = clahe.apply(img_gray)\n",
    "    else:\n",
    "        im = img_gray \n",
    "    \n",
    "    # Setup SimpleBlobDetector parameters.\n",
    "    params = cv2.SimpleBlobDetector_Params()\n",
    "\n",
    "    # Filter by thresholds\n",
    "    params.filterByColor = True\n",
    "    params.blobColor = 0\n",
    "    params.minThreshold = 0\n",
    "    params.maxThreshold = 255\n",
    "    \n",
    "    # Filter by Area.\n",
    "    params.filterByArea = True\n",
    "    params.minArea = (im_height*im_width)*33/1000 # Pixel limit for analysis\n",
    "    params.maxArea = (im_height*im_width)*660/1000\n",
    "    \n",
    "    # Filter by Circularity\n",
    "    params.filterByCircularity = True\n",
    "    params.minCircularity = 0.1\n",
    "    \n",
    "    # Filter by Convexity\n",
    "    params.filterByConvexity = True\n",
    "    params.minConvexity = 0.1\n",
    "    \n",
    "    # Filter by Inertia\n",
    "    params.filterByInertia = True\n",
    "    params.minInertiaRatio = 0.1\n",
    "    \n",
    "    # Create a detector with the parameters\n",
    "    ver = (cv2.__version__).split('.')\n",
    "    if int(ver[0]) < 3 :\n",
    "        detector = cv2.SimpleBlobDetector(params) #Command for Python 2.7\n",
    "    else : \n",
    "        detector = cv2.SimpleBlobDetector_create(params) #Command for Python 3.5\n",
    "        \n",
    "    keyPoints = detector.detect(im)\n",
    "    n_blobs = len(keyPoints)\n",
    "    ROI_blobs = np.zeros((n_blobs,3),np.uint64)\n",
    "    #i is the index of the blob you want to get the position\n",
    "    i=0\n",
    "    for keyPoint in keyPoints:\n",
    "        ROI_blobs[i,0] = keyPoint.pt[0]  #Blob X coordinate\n",
    "        ROI_blobs[i,1] = keyPoint.pt[1]  #Blob Y coordinate\n",
    "        ROI_blobs[i,2] = keyPoint.size   #Blob diameter (average)\n",
    "        i+=1\n",
    "    \n",
    "    # Draw detected blobs as red circles.\n",
    "    # Note that cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS ensures the size of the circle corresponds to the size of blob\n",
    "    im_with_keyPoints = cv2.drawKeypoints(im, keyPoints, np.array([]), (255,255,0), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    \n",
    "    return ROI_blobs, n_blobs, im_with_keyPoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here an example of how the parameters in the previous function modifies the mole detection output (simplified)\n",
    "![Example of Blob detection parameters to adjust for mole detection]\n",
    "![](./src/notebook_imgs/BlobParam.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIDE-FIELD DERMATOLOGICAL SALIENCY ALGORITHM\n",
    "- Based on \"A Model of Saliency-Based Visual Attention for Rapid Scene Analysis\" by Laurent Itti, Christof Koch, and Ernst Niebur. In this algorithm a visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is used. Multiscale image features are combined into a single topographical saliency map created through pigmented lesions collaged into an inconspicous (non-salient) synthetic background created by averaging the original wide-field dermatological image. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail.\n",
    "(Itti, L., Koch, C. and Niebur, E., 1998. A model of saliency-based visual attention for rapid scene analysis. IEEE Transactions on pattern analysis and machine intelligence, 20(11), pp.1254-1259.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define wide-field saliency analysis\n",
    "def wide_field_naive_saliency_analysis(wf_montage_RGB_image, wf_montage_BW_image, marked_wf_orig_image, width=1000):\n",
    "    ## SALIENCY CODE\n",
    "    # Inputs: \n",
    "    #    \"wf_montage_RGB_image\" is a background simplified RGB image of moles to assess color and size differences\n",
    "    #    \"wf_montage_BW_image\" is a background simplified Binarized image of rescaled moles to asses shape differences\n",
    "    #    \"marked_wf_orig_image\" is the wide-field image with previous marks to overlay the results on.\n",
    "    # Outputs:\n",
    "    #    \"wf_overlay_montage_RGB_image\" is an overlayed image with the saliency output\n",
    "    #    \"saliency_img\" is the saliency output   \n",
    "    # Modified from Saliency Code on https://github.com/mayoyamasaki/saliency-map using Laurent Itti, Christof Koch (2000) method\n",
    "    print('Processing Naive Saliency (Ugly Ducking), this may take a while...', end='')\n",
    "    spinner = Spinner()\n",
    "    spinner.start()\n",
    "    \n",
    "    # Analize Ugly Duckling (saliency) considering all Class 3 or above pigmented lesions with resizing for speed\n",
    "    in_sm_c = imutils.resize(wf_montage_RGB_image, width=width)\n",
    "    #in_sm_C = wf_montage_RGB_image\n",
    "    sm_c = SaliencyMap(in_sm_c)\n",
    "    compound_saliency_img = OpencvIo().saliency_array2img([sm_c.map])\n",
    "    \n",
    "    # Analize Ugly Duckling (saliency) considering all Class 3 or above reshaped pigmented lesions with resizing for speed\n",
    "    in_sm_s = imutils.resize(wf_montage_BW_image, width=width)\n",
    "    #in_sm_s = wf_montage_BW_image\n",
    "    sm_s = SaliencyMap(in_sm_s)\n",
    "    shape_saliency_img = OpencvIo().saliency_array2img([sm_s.map])\n",
    "    \n",
    "    # Get main image dimensions for overlay\n",
    "    wf_orig_img_height, wf_orig_img_width = marked_wf_orig_image.shape[:2]\n",
    "    \n",
    "    print(\"Marked original image size: \", wf_orig_img_width, \" X \", wf_orig_img_height)\n",
    "\n",
    "    # Merge saliency maps\n",
    "    saliency_img = cv2.applyColorMap(cv2.addWeighted(compound_saliency_img, 0.75, shape_saliency_img, 0.25, 0), cv2.COLORMAP_JET)\n",
    "    wf_overlay_montage_RGB_image = cv2.addWeighted(marked_wf_orig_image, 0.5, cv2.resize(saliency_img,(wf_orig_img_width, wf_orig_img_height)), 0.5, 0)\n",
    "    spinner.stop()\n",
    "    \n",
    "    return wf_overlay_montage_RGB_image, saliency_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define multiscale spl id and classification function using OPENCV's  blob detection and CNN classifier\n",
    "def multiscale_wide_field_spl_analysis(wf_orig_image, model, im_dim=[img_width,img_height], layer_name = 'dense', display_plots=False):\n",
    "    \n",
    "    # Text defaults for images\n",
    "    font          = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    CornerOfText  = (10,20)\n",
    "    fontScale     = 0.75\n",
    "    fontColor     = (255,255,255)\n",
    "    lineType      = 1\n",
    "    # Box line settings\n",
    "    bbox_line_width = 25\n",
    "\n",
    "    # Specify Display window settings\n",
    "    if display_plots==True:\n",
    "        cv2.namedWindow(\"SPLWindow\", cv2.WINDOW_NORMAL)        # Suspicious Pigmented Lesion Tracking window\n",
    "        cv2.moveWindow(\"SPLWindow\", 0,20)\n",
    "        \n",
    "        cv2.namedWindow(\"MoleWindow\", cv2.WINDOW_NORMAL)       # Mole detection window\n",
    "        cv2.moveWindow(\"MoleWindow\", 0,360)\n",
    "        \n",
    "        cv2.namedWindow(\"SLAWindow\", cv2.WINDOW_NORMAL)        # Single-Lesion Analysis Window\n",
    "        cv2.moveWindow(\"SLAWindow\", 0,695)\n",
    "        \n",
    "        cv2.namedWindow(\"CAMWindow\", cv2.WINDOW_NORMAL)        # Convolutional Activation Map (single-lesion crop)\n",
    "        cv2.moveWindow(\"CAMWindow\", 405,20)\n",
    "\n",
    "        cv2.namedWindow(\"MASWindow\", cv2.WINDOW_NORMAL)        # Mask window (single-lesion) for saliency analaysis\n",
    "        cv2.moveWindow(\"MASWindow\", 405,360)\n",
    "\n",
    "        \n",
    "        # Text defaults for images\n",
    "        font          = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        CornerOfText  = (10,20)\n",
    "        fontScale     = 0.75\n",
    "        fontColor     = (255,255,255)\n",
    "        lineType      = 1\n",
    "        \n",
    "        # Box line settings\n",
    "        bbox_line_width = 25\n",
    "    \n",
    "    # Make copy for marking and Get dimensions (height, width) of wide field image\n",
    "    marked_wf_orig_image = wf_orig_image.copy()\n",
    "    wf_orig_img_height, wf_orig_img_width = wf_orig_image.shape[:2]\n",
    "    #Create Blank RGB and Grayscalemontages\n",
    "    wf_montage_RGB_image = np.ones((wf_orig_img_height,wf_orig_img_width,3),np.uint8)\n",
    "    wf_montage_BW_image = np.zeros((wf_orig_img_height,wf_orig_img_width,3), np.float32)  # Create montage for size, shape combination saliency analysis\n",
    "    \n",
    "    # Fill montage image with image average color(set each pixel to the same value)\n",
    "    avg_RGB = np.uint8(np.mean(wf_orig_image, axis=(0, 1)))\n",
    "    wf_montage_RGB_image[:] = (avg_RGB[0], avg_RGB[1], avg_RGB[2])  # Create montage for color, size, shape combination saliency analysis\n",
    "    \n",
    "    #Initialize Heatmaps for macro image\n",
    "    wf_conv_heatmap = np.zeros((wf_orig_img_height,wf_orig_img_width,3),np.uint8) #Convolutional ACTIVATION HEATMAP\n",
    "    wf_overlay_conv_heatmap = np.zeros((wf_orig_img_height,wf_orig_img_width,3),np.uint8)\n",
    "\n",
    "    # Extract SWIFT Blobs as seeds for SPL analysis and display\n",
    "    (ROI_blobs, n_blobs, im_with_keypoints) = get_multiscale_moles(wf_orig_image, CLAHE_Adj = False)\n",
    "    \n",
    "    # Adjust image if it is not mostly skin,  naive algorithm (0.9 is the threshold)\n",
    "    skin_percent = np.sum(skin_detector(wf_orig_image).astype(int))/(wf_orig_img_height*wf_orig_img_width*255)\n",
    "    print(\"Skin percentage in image: \" + str(skin_percent))\n",
    "    if(skin_percent>0.75)and(skin_percent<=0.8):\n",
    "        print(\"Adjusting\")\n",
    "        wf_orig_image = color_balance(wf_orig_image, 1)\n",
    "        wf_orig_image = adjust_gamma(wf_orig_image)\n",
    "        wf_orig_image = apply_clahe(wf_orig_image, c_lim=1.0)\n",
    "    elif (skin_percent<=0.75):\n",
    "        print(\"Adjusting\")\n",
    "        #wf_orig_image = color_balance(wf_orig_image, 1)\n",
    "        #wf_orig_image = adjust_gamma(wf_orig_image,gamma=1.75)\n",
    "        wf_orig_image = apply_clahe(wf_orig_image, c_lim=0.25)\n",
    "        \n",
    "    elif (skin_percent>0.8):\n",
    "        print(\"Adjusting\")\n",
    "        #wf_orig_image = color_balance(wf_orig_image, 1)\n",
    "\n",
    "    \n",
    "    if display_plots==True:\n",
    "        cv2.imshow(\"SLAWindow\", wf_orig_image)\n",
    "        cv2.imshow(\"MoleWindow\", im_with_keypoints)\n",
    "    \n",
    "    # TQDM Progressbar\n",
    "    pbar = tqdm(total=n_blobs)\n",
    "\n",
    "    # Loop over for each pigmented lesion for analysis\n",
    "    n_splf = 0 #Counter of non-malignant SPLs to follow\n",
    "    n_splm = 0 #Counter of possibly malignant SPLs\n",
    "    im_pls = []  # initialize the list of pigmented lesion image\n",
    "    f_win =1.5\n",
    "    \n",
    "    # Define the paths for the temporary folders and check if they exist, if so purge them then recreate as empty\n",
    "\n",
    "    blob_temp_folder_path = 'app/output/analysis/Ugly_Duckling_Analysis/Blobs/'\n",
    "    if os.path.exists(blob_temp_folder_path):\n",
    "        shutil.rmtree(blob_temp_folder_path)\n",
    "    os.makedirs(blob_temp_folder_path)\n",
    "\n",
    "    pl_temp_folder_path = 'app/output/analysis/Ugly_Duckling_Analysis/Pigmented_Lesions/'\n",
    "    if os.path.exists(pl_temp_folder_path):\n",
    "        shutil.rmtree(pl_temp_folder_path)\n",
    "    os.makedirs(pl_temp_folder_path)\n",
    "    \n",
    "    orig_coordinates = np.empty((0,4))\n",
    "    resized_coordinates = np.empty((0,4))\n",
    "    ROI_PLs = np.empty((0,3))\n",
    "    n_blob_prop= np.empty((0,1))\n",
    "    \n",
    "    for blob_id in range(0, n_blobs):\n",
    "        # Get centroid coordinates and diameter of each pigmented lesion (PL) and calculate bounding box x0,x1,y0,y1\n",
    "        (c_x, c_y, c_d) = ROI_blobs[blob_id,:]\n",
    "        # We make every bounding box 3x the diameter of the lesion to account for high eccentricity\n",
    "        x0 = np.uint64(max(0, c_x-f_win*c_d))\n",
    "        y0 = np.uint64(max(0, c_y-f_win*c_d))\n",
    "        x1 = np.uint64(max(0, c_x+f_win*c_d))\n",
    "        y1 = np.uint64(max(0, c_y+f_win*c_d))\n",
    "        orig_coordinates = np.vstack((orig_coordinates, np.array([x0, y0, x1, y1])))\n",
    "        \n",
    "        # Crop PL over wide field image\n",
    "        crop_img = wf_orig_image[y0:y1, x0:x1] \n",
    "        \n",
    "        # Save Blob images for later analysis\n",
    "        crop_blob_img_file_path = blob_temp_folder_path + 'B_' + str(blob_id) + '.png' \n",
    "        cv2.imwrite(crop_blob_img_file_path,crop_img)\n",
    "        \n",
    "        #Get image crop size\n",
    "        (crop_img_width , crop_img_height) = crop_img.shape[:2]\n",
    "        \n",
    "        # Create RGB crop with unmodified lesion segmentation\n",
    "        masked_crop_RGB_img = np.zeros((crop_img_width,crop_img_height,3),np.uint8)\n",
    "\n",
    "        # Resize image\n",
    "        eval_img = cv2.resize(crop_img,(im_dim[0], im_dim[1]))\n",
    "        \n",
    "        # Extract SWIFT Blobs as seeds for SPL analysis and display\n",
    "        (eval_img_ROI_blobs, eval_img_n_blobs, eval_img_im_with_keypoints) = get_center_mole(eval_img, CLAHE_Adj = False)\n",
    "        #if eval_img_n_blobs > 0:\n",
    "        #    np.max(eval_img_ROI_blobs[:,2])\n",
    "\n",
    "        #Classify pigmented lesion (analyze shot, classify and display Convolutional heatmap with class )\n",
    "        img_tensor = img_to_array(eval_img)                    # (height, width, channel\n",
    "        img_tensor = np.expand_dims(img_tensor, axis=0)         # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels)\n",
    "        img_tensor /= 255.                                      # imshow expects values in the range [0, 1]\n",
    "        img_scores = model.predict(img_tensor)\n",
    "        predicted_class = img_scores.argmax(axis=-1)\n",
    "        \n",
    "        #Check if skin is detected by naive algorithm (0.001 is the threshold)\n",
    "        skin_mole_percent = np.sum(skin_detector(eval_img).astype(int))/(im_dim[0]*im_dim[1]*255)\n",
    "        #print(img_scores)\n",
    "        #print(skin_mole_percent)\n",
    "        \n",
    "        ## *************************************** SKIN/MOLE CHECK SECTION ********************************* \n",
    "        #Check if skin/mole is detected by confirmatory naive algorithm (to reduce false positives)\n",
    "        th_1=1.0\n",
    "        th_2=0.995\n",
    "        if predicted_class==0:\n",
    "            if (skin_mole_percent>=th_1) and (eval_img_n_blobs>=1):\n",
    "                print('changed from ' + str(predicted_class) + ' to ')\n",
    "                predicted_class = np.array([3])\n",
    "        \n",
    "        elif predicted_class==1:\n",
    "            if (skin_mole_percent>=th_2) and (eval_img_n_blobs>=1):\n",
    "                print('changed from ' + str(predicted_class) + ' to ')\n",
    "                predicted_class = np.array([3])\n",
    "                \n",
    "        elif predicted_class==2:\n",
    "            if (skin_mole_percent>=th_1) and (eval_img_n_blobs>=1):\n",
    "                print('changed from ' + str(predicted_class) + ' to ')\n",
    "                predicted_class = np.array([3])\n",
    "\n",
    "        elif predicted_class>=3:\n",
    "            if (eval_img_n_blobs<1):\n",
    "                print('changed from ' + str(predicted_class) + ' to ')\n",
    "                predicted_class = np.array([2])\n",
    "                \n",
    "            elif (eval_img_n_blobs>=1):\n",
    "                if (skin_mole_percent<=th_1/8):\n",
    "                    print('changed from ' + str(predicted_class) + ' to ')\n",
    "                    predicted_class = np.array([0])\n",
    "                elif (skin_mole_percent<=th_1/2):\n",
    "                    print('changed from ' + str(predicted_class) + ' to ')\n",
    "                    predicted_class = np.array([1])\n",
    "  \n",
    "        #print(predicted_class)\n",
    "        ## *************************************** END OF SECTION ******************************\n",
    "        \n",
    "        \n",
    "        # Display the Macro Window of the sliding process\n",
    "        if predicted_class == 4:\n",
    "            n_splf +=1\n",
    "            marked_wf_orig_image = marked_wf_orig_image.copy()\n",
    "            cv2.rectangle(marked_wf_orig_image, (x0, y0), (x1, y1), (0, 255, 255), bbox_line_width)\n",
    "            cv2.imshow(\"SLAWindow\", marked_wf_orig_image)\n",
    "        elif predicted_class == 5:\n",
    "            n_splm +=1\n",
    "            marked_wf_orig_image = marked_wf_orig_image.copy()\n",
    "            cv2.rectangle(marked_wf_orig_image, (x0, y0), (x1, y1), (0, 0, 255), bbox_line_width)\n",
    "            cv2.imshow(\"SLAWindow\", marked_wf_orig_image)\n",
    "        \n",
    "        if predicted_class >=3:\n",
    "            # Save only potential PL images for later analysis\n",
    "            crop_pl_img_file_path = pl_temp_folder_path + 'P_' + str(blob_id) + '.png' \n",
    "            cv2.imwrite(crop_pl_img_file_path,crop_img)\n",
    "            \n",
    "            #Populate new ROI_PLs variable with ROI_blobs values of selected pigmented lesions\n",
    "            ROI_PLs = np.vstack((ROI_PLs, ROI_blobs[blob_id]))\n",
    "            \n",
    "            #Track blob size\n",
    "            if eval_img_n_blobs>0:\n",
    "                n_blob_prop = np.vstack((n_blob_prop, (eval_img_ROI_blobs[0,2]/(im_dim[0]*im_dim[1]))*100))\n",
    "            else:\n",
    "                n_blob_prop = np.vstack((n_blob_prop, 0))\n",
    "            \n",
    "            #Append images\n",
    "            im_pls.append(crop_img)\n",
    "            crop_gray = cv2.cvtColor(crop_img,cv2.COLOR_BGR2HSV)[:,:,1] #Select saturation channel which is great for skn detection\n",
    "        \n",
    "            # Otsu's thresholding with optiona Gaussian filtering\n",
    "            # crop_blur = cv2.GaussianBlur(crop_gray,(5,5),0)\n",
    "            thres, mask = cv2.threshold(crop_gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "                        \n",
    "            # Fill RGB crop with unmodified lesion segmentation\n",
    "            masked_crop_RGB_img = cv2.bitwise_and(crop_img, crop_img, mask=mask)\n",
    "            # Add segmented RGB crop of original size to montage for compound saliency\n",
    "            wf_montage_RGB_image[y0:y1,x0:x1,:] = cv2.bitwise_and(wf_montage_RGB_image[y0:y1,x0:x1,:], wf_montage_RGB_image[y0:y1,x0:x1,:], mask = cv2.bitwise_not(mask)) + masked_crop_RGB_img\n",
    "            \n",
    "            # Create BW crop with re-scaled binary (0 or 255) lesion segmentation\n",
    "            masked_crop_BW_img = np.copy(masked_crop_RGB_img)      # Clone RGB crop imag\n",
    "            masked_crop_BW_img[masked_crop_BW_img > 0] = 255\n",
    "            # Add segmented and Resized BW crop to montage for shape-only saliency\n",
    "            c_rd = round(np.mean(ROI_blobs[:,2]))\n",
    "            rx0 = np.uint64(max(0, c_x-f_win*c_rd))\n",
    "            ry0 = np.uint64(max(0, c_y-f_win*c_rd))\n",
    "            rx1 = np.uint64(max(0, c_x+f_win*c_rd))\n",
    "            ry1 = np.uint64(max(0, c_y+f_win*c_rd))\n",
    "            resized_coordinates = np.vstack((resized_coordinates, np.array([rx0, ry0, rx1, ry1])))\n",
    "            im_rdim = wf_montage_BW_image[ry0:ry1,rx0:rx1,:].shape\n",
    "            \n",
    "            masked_crop_BW_resize_img = cv2.resize(masked_crop_BW_img,(im_rdim[1],im_rdim[0])) # Re-scale to accelerate  \n",
    "            wf_montage_BW_image[ry0:ry1,rx0:rx1,:] = wf_montage_BW_image[ry0:ry1,rx0:rx1,:] + masked_crop_BW_resize_img\n",
    "\n",
    "        # Process and Display CNN output for each window\n",
    "        layer_idx = [idx for idx, layer in enumerate(model.layers) if layer.name == layer_name][0]\n",
    "\n",
    "        gradcam = Gradcam(model)\n",
    "\n",
    "        # Define the target score (e.g., class 0 or any specific class)\n",
    "        # For ImageNet, class index 0 is \"tench\" (a type of fish)\n",
    "        # You can change this index based on your specific target class.\n",
    "        score = CategoricalScore(0)\n",
    "\n",
    "        # Generate the heatmap\n",
    "        heatmap = gradcam(score, img_preprocessed, penultimate_layer=layer_idx)  # The last convolutional layer is used here\n",
    "\n",
    "        # Visualize the heatmap (Grad-CAM)\n",
    "        heatmap = np.squeeze(heatmap) \n",
    "\n",
    "        eval_conv_heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        eval_overlay_conv_heatmap = cv2.putText(eval_conv_heatmap,('Class: '+str(predicted_class)), CornerOfText, font, fontScale, fontColor, lineType)\n",
    "        \n",
    "        # Construct rescaled heatmap with dimensions (height, width) of original crop  image\n",
    "        crop_conv_heatmap = cv2.resize(eval_overlay_conv_heatmap,(crop_img_height, crop_img_width))\n",
    "            \n",
    "        # Convert the image to float32\n",
    "        wf_conv_heatmap = wf_conv_heatmap.astype(np.float32)\n",
    "\n",
    "        # Optionally, scale pixel values to the range [0, 1] if required (common for float images)\n",
    "        wf_conv_heatmap /= 255.0\n",
    "        print(\"Crop Convolutional Heatmap shape: \", crop_conv_heatmap.shape, \" TYPE: \", crop_conv_heatmap.dtype);\n",
    "        print(\"WF conv Heatmap shape: \", wf_conv_heatmap.shape, \" TYPE: \", wf_conv_heatmap.dtype);\n",
    "\n",
    "        wf_orig_image = wf_orig_image.astype(np.float32)\n",
    "        wf_orig_image /= 255.0\n",
    "        \n",
    "        # Stitch CNN output for macro image display\n",
    "        wf_conv_heatmap[y0:y1,x0:x1,:] = cv2.addWeighted(wf_conv_heatmap[y0:y1,x0:x1,:], 0.5, crop_conv_heatmap, 0.5,  0)\n",
    "        wf_overlay_conv_heatmap = cv2.addWeighted(wf_orig_image, 1.0, wf_conv_heatmap, 0.5, 0)\n",
    "\n",
    "    \n",
    "        # Display SPL and wide-field CNN outputs\n",
    "        if display_plots==True:\n",
    "            print(\"Displaying plots... wait for key press\")\n",
    "            cv2.imshow(\"CAMWindow\", eval_overlay_conv_heatmap) # Display class filter with single lesions\n",
    "            cv2.imshow(\"MASWindow\", masked_crop_RGB_img) # Display mask for shape saliency analysis (masked_crop_RGB_img or eval_img_im_with_keypoints)\n",
    "            cv2.imshow(\"SPLWindow\", wf_overlay_conv_heatmap)   # Display class filter with all lesions\n",
    "            cv2.waitKey(1)\n",
    "            time.sleep(0.025)\n",
    "        \n",
    "        pbar.update(1)\n",
    "    \n",
    "    # NAIVE SALIENCY FUNCTION\n",
    "    wf_overlay_montage_RGB_image, saliency_img = wide_field_naive_saliency_analysis(wf_montage_RGB_image, wf_montage_BW_image, marked_wf_orig_image, width=1000)\n",
    "\n",
    "    # Display  \n",
    "    if display_plots==True:\n",
    "        \n",
    "        #Create saliency windows\n",
    "        cv2.namedWindow(\"SALWindow\", cv2.WINDOW_NORMAL)        # Create window with freedom of dimensions\n",
    "        cv2.moveWindow(\"SALWindow\", 405,695)\n",
    "        \n",
    "        # Display Ugly Duckling Analysis (Saliency)\n",
    "        cv2.imshow('SALWindow',wf_overlay_montage_RGB_image)\n",
    "        cv2.waitKey(1)\n",
    "    \n",
    "    print('Analysis Completed!')\n",
    "    \n",
    "    # Close process bar\n",
    "    pbar.close()\n",
    "    \n",
    "    return n_splm, n_splf, ROI_PLs, n_blobs, n_blob_prop, marked_wf_orig_image, wf_overlay_conv_heatmap, im_with_keypoints, im_pls, wf_montage_RGB_image, wf_montage_BW_image, saliency_img, wf_overlay_montage_RGB_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify single wide-field-image from folder to be processed.\n",
    "\n",
    "def get_image_path(images_path, image_num):\n",
    "    #Get image path for a specific image in a specified folder of images\n",
    "    image_files = sorted([file_name for file_name in os.listdir(images_path)\n",
    "        if file_name.lower().endswith(('.png', '.jpeg', '.tiff'))])\n",
    "    if len(image_files) == 0:\n",
    "        raise FileNotFoundError(f\"No images found at: {images_path}\")\n",
    "    if image_num < 0 or image_num >= len(image_files):\n",
    "        raise IndexError(f\"Index {image_num} is out of range.\")\n",
    "    return os.path.join(images_path, image_files[image_num])\n",
    "\n",
    "# img_path ='app/data/ovio/45981251_078.jpg'\n",
    "#img_path = '/app/data/examples/wide_field_images/' + img_name + '.tiff'\n",
    "#img_path ='data/wide_field_database/UglyDucklingTest/Examples/Wide_Field_Images/DSC_5443.png'\n",
    "#img_path ='data/wide_field_database/UglyDucklingTest/Examples/Wide_Field_Images/DSC_5444.tiff'\n",
    "#img_path ='data/wide_field_database/UglyDucklingTest/Examples/Wide_Field_Images/DSC_5445.tiff'\n",
    "img_path = get_image_path('/app/data/examples/wide_field_images/', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wide field image\n",
    "wf_orig_image_os = cv2.imread(img_path)\n",
    "\n",
    "# Check if full image is too blured for analysis (IF ALL GOOD CONTINUE)\n",
    "blur_threshold = int(math.sqrt(np.shape(wf_orig_image_os)[0]*np.shape(wf_orig_image_os)[1])/1000)\n",
    "wf_orig_image_gray = cv2.cvtColor(wf_orig_image_os, cv2.COLOR_BGR2GRAY) # if the focus measure is less than the supplied threshold,\n",
    "blur_detector = variance_of_laplacian(wf_orig_image_gray)    # then the image should be considered \"blurry\"\n",
    "print('Blur score: ' + str(blur_detector))\n",
    "print('Blur threshold: ' + str(blur_threshold))\n",
    "if blur_detector < blur_threshold:\n",
    "    print('Input image too blurred for analysis')\n",
    "    sys.exit(\"Blured image!\")\n",
    "\n",
    "# Adjust image\n",
    "wf_orig_image = wf_orig_image_os\n",
    "wf_orig_image = cv2.resize(wf_orig_image, (img_width, img_height));\n",
    "# Perform multiscale spl id and classification using OPENCV's blob detection and CNN classifier\n",
    "(n_splm, n_splf, ROI_PLs, n_blobs, n_blob_prop, marked_wf_orig_image, wf_overlay_conv_heatmap, \n",
    " im_with_keypoints, im_pls, wf_montage_RGB_image, wf_montage_BW_image, \n",
    " saliency_img, wf_overlay_montage_RGB_image) = multiscale_wide_field_spl_analysis(wf_orig_image, model, im_dim=[img_width, img_height], layer_name = layer_name, display_plots=False)\n",
    "\n",
    "#Save all image outputs to disk\n",
    "output_dir = 'app/output/analysis/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "cv2.imwrite(os.path.join(output_dir, 'wf_orig_image_os.png'), wf_orig_image_os)\n",
    "cv2.imwrite(os.path.join(output_dir, 'wf_orig_image.png'), wf_orig_image)\n",
    "cv2.imwrite(os.path.join(output_dir, 'im_with_keypoints.png'), im_with_keypoints)\n",
    "cv2.imwrite(os.path.join(output_dir, 'marked_wf_orig_image.png'), marked_wf_orig_image)\n",
    "cv2.imwrite(os.path.join(output_dir, 'wf_overlay_conv_heatmap.png'), wf_overlay_conv_heatmap)\n",
    "cv2.imwrite(os.path.join(output_dir, 'wf_montage_RGB_image.png'), wf_montage_RGB_image)\n",
    "cv2.imwrite(os.path.join(output_dir, 'wf_montage_BW_image.png'), wf_montage_BW_image)\n",
    "cv2.imwrite(os.path.join(output_dir, 'naive_saliency_img.png'), saliency_img)\n",
    "cv2.imwrite(os.path.join(output_dir, 'naive_saliency_overlay_img.png'), wf_overlay_montage_RGB_image)\n",
    "\n",
    "#Print results\n",
    "print('According to Blob Detection algorithm: ')\n",
    "print('---> Analyzed Taget Regions: ' + str(n_blobs))\n",
    "print('---> Detected Pigmented Lesions: ' + str(len(im_pls)))\n",
    "print('According to CNN Classifier: ')\n",
    "print('---> Mildly Suspicious (Consider Following): ' + str(n_splf))\n",
    "print('---> Highly Suspicious (Consider Biopsy): ' + str(n_splm))\n",
    "print('---> One-shot ODDNESS of most salient regions is showed')\n",
    "print('---> This region of the patient is: ',end='')\n",
    "if ((len(im_pls)<20) and (n_splf==0) and (n_splm==0)):\n",
    "    print('LOW RISK')\n",
    "elif (((20<=len(im_pls)) and (len(im_pls)<50) or (n_splf<=3)) and (n_splm==0)):\n",
    "    print('MEDIUM RISK')\n",
    "elif ((len(im_pls)>=50) or (n_splf>=3) or (n_splm>=1)):\n",
    "    print('HIGH RISK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN FEATURE-BASED UGLY DUCKLING ASSESMENT & t-SNE\n",
    "## (with reverse image search and retrieval as examples)\n",
    "\n",
    "This notebook will show you how you can use a convolutional neural network (convnet) to search through a large collection of images. Specifically, it will show you how you can retrieve a set of images which are similar to a query image, returning you its `n` nearest neighbors in terms of image content. Based on code by ml4a (https://github.com/ml4a/ml4a-guides/blob/master/notebooks/image-search.ipynb)\n",
    "\n",
    "### Prepare intra-patient Pigmented lesion dataset\n",
    "\n",
    "Finally, prepare a folder of images per patient to do the analysis on. Each folder will contain roughly 100 images. Run the following commands inside a folder of your choosing (this notebook will assume you do so in the `output/analysis/Ugly_Duckling_Analysis/Pigmented_Lesions/` folder.\n",
    "\n",
    "Now we can begin. Re-run the import commands on TOP to make sure all the libraries are correctly installed and import without errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our VGG16 implementation using transfer learning was the best performing in the CNN analysis will load a AGAIN THE pre-trained neural network VGG16, which comes with Keras. If it's your first time loading it, it will automatically download the weights for you, which will take some time. Afterwards, the weights will be cached locally for future use. Keras has a number of other [pre-trained networs](https://keras.io/applications/) which you can try as well.\n",
    "\n",
    "Once the network is loaded, we can take a look at the architecture of the network by running `model.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This imports VGG16 with all weights from Imagenet training since VGG16 architecture \n",
    "model = applications.VGG16(include_top=True, weights='imagenet')\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous summary gives us a layer-by-layer description of the network. Notice that VGG16 is a deep network with 13 convolutional layers. It was previously trained on millions of images, and has over 100,000,000 weights and biases, the majority of which connect to the first fully-connected layer (fc1).\n",
    "\n",
    "To see it in action with our pigmented lesion dataset, let's load an image and input it into the network. To help us do this, we will create a function get_image(path) which will handle the usual pre-processing steps: load an image from our file system and turn it into an input vector of the correct dimensions, those expected by VGG16, namely a color image of size 224x224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define first a get_image function that will return a handle to the image itself, and a numpy array of its pixels to input the network\n",
    "def get_image(path):\n",
    "    img = image.load_img(path, target_size=model.input_shape[1:3])\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return img, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load an image into memory, convert it into an input vector, and see the model's top 5 predictions for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single SPL Example \n",
    "#img_path ='original_data/0_background/0_BKGR_000088.png'\n",
    "#img_path ='original_data/1_skinedge/1_P134_00484.png'\n",
    "#img_path ='original_data/2_skin/2_P004_00005b.png'\n",
    "#img_path ='original_data/3_nspl/3_P056_00009_03.png'\n",
    "#img_path ='original_data/4_nspl_to_follow/4_Atypical_Benign_000053.png'\n",
    "img_path = 'app/data/examples/single_mole_images/5_Melanoma_0002.png'\n",
    "img, x = get_image(img_path)\n",
    "predictions = model.predict(x)\n",
    "plt.axis(\"off\")\n",
    "imshow(img)\n",
    "for pred in decode_predictions(predictions)[0]:\n",
    "    print(\"predicted %s with probability %0.3f\" % (pred[1], pred[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remove the top classification layer from our network, leaving the last fully-connected layer, \"fc2 (Dense)\" as the new output layer. The way we do this is by instantiating a new model called `feature_extractor` which takes a reference to the desired input and output layers in our VGG16 model. Thus, `feature_extractor`'s output is the layer just before the classification, the last 4096-neuron fully connected layer. \n",
    "\n",
    "Note about memory: although we instantiate a new object with most of the weights of our large model, it does not actually duplicate all the weights of the previous network into memory. \n",
    "\n",
    "If we run the `summary()` function again, we see that the architecture of `feat_extractor` is identical to the original `model`, except the last layer has been removed. We also know that not just the architecture is the same, but the two have the same weights as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all features from dense layer in VGG16\n",
    "feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\n",
    "feat_extractor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the feature extractor in action. We pass the same image from before into it, and look at the results. The `predict` function returns an array with one element per image (in our case, there is just one). Each element contains a 4096-element array, which is the activations of the last fully-connected layer in VGG16. Let's plot the array as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize all feature values of a single PL\n",
    "img, x = get_image(img_path)\n",
    "feat = feat_extractor.predict(x)\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(feat[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load all of the images in a directory, and use `feature_extractor` to get a feature vector for each one. If you have your own folder of images you want to analyze, change the path of `images_path` and it will search recursively through all the folders in it. Set `max_num_images` to cap it at some maximum number of images to load (it will grab a random subset of `max_num_images` is less than the number of images in your directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all images to analyze for ugly duckling\n",
    "images_path = 'app/output/analysis/Ugly_Duckling_Analysis/Pigmented_Lesions/'\n",
    "images = [os.path.join(dp, f) for dp, dn, filenames in os.walk(images_path) for f in filenames if os.path.splitext(f)[1].lower() in ['.jpg','.png','.jpeg']]\n",
    "images = sorted(images, key = lambda x: int(x.split(\"/P_\")[-1].split(\".\")[0]))\n",
    "print(\"keeping %d images to analyze\" % len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part will take the longest. We iterate through and extract the features from all the images in our `images` array, placing them into an array called `features`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for all images\n",
    "features = []\n",
    "for image_path in tqdm(images):\n",
    "    img, x = get_image(image_path);\n",
    "    feat = feat_extractor.predict(x)[0]\n",
    "    features.append(feat)\n",
    "\n",
    "#Add relative size of image as part of vector\n",
    "min_max_scaler = MinMaxScaler()\n",
    "pl_size_norm = min_max_scaler.fit_transform(np.expand_dims(ROI_PLs[:,2], axis=1))* np.max(features)\n",
    "features=np.append(features, pl_size_norm,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save all features without any dimensionallity reduction\n",
    "pickle.dump([images, features, ROI_PLs], open('output/analysis/Ugly_Duckling_Analysis/Output/features_IntraPatient_PLs.p', 'wb'))\n",
    "np.savetxt(\"output/analysis/Ugly_Duckling_Analysis/Output/features_IntraPatient_PLs.tsv\", features, delimiter='\\t', newline='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that is done, we will take our `n`x4096 matrix of features (where `n` is the number of images), and apply [principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) to it, and keep the first 300 principal components, creating an `n`x300 matrix called `pca_features`. \n",
    "\n",
    "The purpose of principal component analysis is to reduce the dimensionality of our feature vectors. This reduces the amount of redundancy in our features (from duplicate or highly-correlated features), speeds up computation over them, and reduces the amount of memory they take up. \n",
    "\n",
    "\n",
    "\n",
    "Let's do a query. What we'll do is define a function which returns the num_results closest images to a query image, with repsect to those images contents. What it dos is: for the given query image, it will take its PCA-activations, and compute the euclidean distance between it and every other set of PCA-activations, then return the best ones.\n",
    "We also define a helper function get_concatenated_images which creates a thumbnail of a set of images, so we can display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a PCA analysis on features as example for possible handling with 300 dimensions\n",
    "features = np.array(features)\n",
    "pca = PCA()\n",
    "pca.fit(features)\n",
    "pca_features = pca.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to do our reverse image queries! The matrix `pca_features` contains a compact representation of our images, one 300-element row for each image with high-level feature detections. We should expect that two similar images, which have similar content in them, should have similar arrays in `pca_features`.\n",
    "\n",
    "Thus we can define a new function `get_closest_images`, which will compute the euclidean distance between the PCA features of `query_image_idx`-th image in our dataset, and the PCA features of every image in the dataset (including itself, trivially 0). It then returns an array of indices to the `num_results` (default is 5) most similar images to it (not including itself). \n",
    "\n",
    "We also define a helper function `get_concatenated_images` which will simply take those resulting images and concatenate them into a single image for easy display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_images(query_image_idx, num_results=5):\n",
    "    distances = [ distance.euclidean(pca_features[query_image_idx], feat) for feat in pca_features ]\n",
    "    idx_closest = sorted(range(len(distances)), key=lambda k: distances[k])[1:num_results+1]\n",
    "    return idx_closest\n",
    "\n",
    "def get_concatenated_images(indexes, thumb_height):\n",
    "    thumbs = []\n",
    "    for idx in indexes:\n",
    "        img = image.load_img(images[idx])\n",
    "        img = img.resize((int(img.width * thumb_height / img.height), thumb_height))\n",
    "        thumbs.append(img)\n",
    "    concat_image = np.concatenate([np.asarray(t) for t in thumbs], axis=1)\n",
    "    return concat_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a query on a randomly selected image in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a query on a random image\n",
    "query_image_idx = int(len(images) * random.random())\n",
    "idx_closest = get_closest_images(query_image_idx)\n",
    "query_image = get_concatenated_images([query_image_idx], 300)\n",
    "results_image = get_concatenated_images(idx_closest, 200)\n",
    "\n",
    "# display the query image\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.axis(\"off\")\n",
    "imshow(query_image)\n",
    "plt.title(\"query image (%d)\" % query_image_idx)\n",
    "\n",
    "# display the resulting images\n",
    "plt.figure(figsize = (16,12))\n",
    "plt.axis(\"off\")\n",
    "imshow(results_image)\n",
    "plt.title(\"result images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are satisfied with the quality of our image vectors, now would be a good time to save them to disk for later usage. You will need these vectors to run the [next notebook on making an image t-SNE](image-tsne.ipynb).\n",
    "\n",
    "We need to save both the image features matrix (the PCA-reduced features, not the originals), as well as the array containing the paths to each image, to make sure we can line up the images to their corresponding vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save PCA-reduced features with N-components (300)\n",
    "pickle.dump([images, pca_features, ROI_PLs], open('output/analysis/Ugly_Duckling_Analysis/Output/pca/pca_features_n300_IntraPatient_PLs.p', 'wb'))\n",
    "np.savetxt(\"output/analysis/Ugly_Duckling_Analysis/Output/pca/pca_features_n300_IntraPatient_PLs.tsv\", pca_features, delimiter='\\t', newline='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing you can try is to do is fine a path between two images containing `n` images. The below is a naive approach to this problem which finds the closest image to the `n` vectors which are interpolated between those of the endpoint images. A better one would be to use a variant of [Dijkstra's algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm) (implementation TBD). This implementation is not particularly good; improvement TBD (suggestions are welcome!)\n",
    "\n",
    "With the naive approach, we run another principal component analysis, this time reducing down all the way to 3 dimensions. The reason for this is when there are too many dimensions and the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) sets in, most images cluster strongly around their class, and there are few images between classes.  In a low-dimensional space, this isn't as much a problem. So we first run a new PCA, saving the columns to `pca_features_n3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Run a PCA analysis on features as example for possible handling with 3 dimensions\n",
    "    features = np.array(features)\n",
    "    n_components=3\n",
    "    pca_n3 = PCA(n_components=n_components)\n",
    "    pca_n3.fit(features)\n",
    "    pca_features_n3 = pca_n3.transform(features)\n",
    "    \n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define our function `get_image_path_between` which will make `num_hops` sized stops between two images, and grab the closest image at each step, then concatenate them together and display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_path_between(query_image_idx_1, query_image_idx_2, num_hops=1):\n",
    "    path = [query_image_idx_1, query_image_idx_2]\n",
    "    for hop in range(num_hops-1):\n",
    "        t = float(hop+1) / num_hops\n",
    "        lerp_acts = t * pca_features_n3[query_image_idx_1] + (1.0-t) * pca_features_n3[query_image_idx_2]\n",
    "        distances = [distance.euclidean(lerp_acts, feat) for feat in pca_features_n3]\n",
    "        idx_closest = sorted(range(len(distances)), key=lambda k: distances[k])\n",
    "        path.insert(1, [i for i in idx_closest if i not in path][0])\n",
    "    return path\n",
    "\n",
    "try:\n",
    "    # pick image and number of hops\n",
    "    num_hops = 10\n",
    "    max_num_hops = int(np.size(pca_features_n3)/n_components)\n",
    "    \n",
    "    if num_hops >= max_num_hops:\n",
    "        num_hops = max_num_hops-1\n",
    "    \n",
    "    query_image_idx_1 = int(len(images) * random.random())\n",
    "    query_image_idx_2 = int(len(images) * random.random())\n",
    "    \n",
    "    # get path\n",
    "    path = get_image_path_between(query_image_idx_1, query_image_idx_2, num_hops)\n",
    "    \n",
    "    # draw image\n",
    "    path_image = get_concatenated_images(path, 200)\n",
    "    plt.figure(figsize = (16,12))\n",
    "    plt.title(\"Range of intra-patient mole types\")\n",
    "    plt.axis(\"off\")\n",
    "    imshow(path_image)\n",
    "\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #Save PCA-reduced features with N-components (3)\n",
    "    pickle.dump([images, pca_features, ROI_PLs], open('output/analysis/Ugly_Duckling_Analysis/Output/pca/pca_features_n3_IntraPatient_PLs.p', 'wb'))\n",
    "    np.savetxt(\"output/analysis/Ugly_Duckling_Analysis/Output/pca/pca_features_n3_IntraPatient_PLs.tsv\", pca_features_n3, delimiter='\\t', newline='\\n')\n",
    "\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Ranking and Outlier Detection Based on CNN Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES to CHANGE HERE\n",
    "top_percent_thresh = 0.03 # Select the opt x percent of results as outliers\n",
    "\n",
    "# Get Paths to images, features and ROIs from pickle file\n",
    "images, features, ROI_PLs= pickle.load(open('output/analysis/Ugly_Duckling_Analysis/Output/features_IntraPatient_PLs.p', 'rb'))\n",
    "\n",
    "# Find mean of the dataset by finding the point with corresponding coordinate means of each feature for entire dataset\n",
    "origin = np.array([np.mean(features, axis=0)])\n",
    "\n",
    "# Measure distance between origin and all the sample points\n",
    "pairwise_dist = distance.cdist(features, origin, metric='cosine')\n",
    "\n",
    "# Adjust distance using relative size of lesion as 1/4 the components of the ABCD criteria\n",
    "pairwise_dist = min_max_scaler.fit_transform(pairwise_dist)\n",
    "pl_size_norm = min_max_scaler.fit_transform(pl_size_norm)-0.5\n",
    "n_blob_prop = (min_max_scaler.fit_transform(n_blob_prop)-1.0)\n",
    "pairwise_dist = min_max_scaler.fit_transform(pairwise_dist/3 + pl_size_norm/3 + n_blob_prop/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features in right data type\n",
    "odd_scores = np.float64(pairwise_dist)\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0, 255)).fit(odd_scores)\n",
    "rescaled_distances = np.uint8(scaler.transform(odd_scores))\n",
    "uint8_odd_scores=[odd_scores[0] for odd_scores in rescaled_distances]\n",
    "\n",
    "# Transform into Pandas dataframe to make working with the data easier\n",
    "embedding_results = pd.DataFrame({'image':images, 'distance':list(pairwise_dist), 'rescaled_scores':list(uint8_odd_scores)})\n",
    "\n",
    "# Sort values by descending distance\n",
    "sorted_embedding_results = embedding_results.sort_values('distance', ascending=False)\n",
    "\n",
    "#Visualize example of features\n",
    "for i, f in list(zip(images, features))[0:5]:\n",
    "    print(\"image: %s, features: %0.2f,%0.2f,%0.2f,%0.2f... \"%(i, f[0], f[1], f[2], f[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UG ranked Montage Build Functions\n",
    "def images_to_ranked_montage(sorted_embedding_results, output_fn):\n",
    "    #Display settings\n",
    "    row_size = 10\n",
    "    margin = 20\n",
    "    cmap = cm.Reds\n",
    "    #Setup\n",
    "    filenames = sorted_embedding_results.image.values\n",
    "    images = [Image.open(filename) for filename in filenames]\n",
    "    width = max(image.size[0] + margin for image in images)*row_size\n",
    "    height = sum(image.size[1] + margin for image in images)\n",
    "    montage = Image.new(mode='RGBA', size=(width, height), color=(255,255,255,255))\n",
    "    #Initialization\n",
    "    max_x = 0\n",
    "    max_y = 0\n",
    "    offset_x = 0\n",
    "    offset_y = 0\n",
    "    i = 0\n",
    "    #Montage creation\n",
    "    for index,image in enumerate(images):\n",
    "        #Modify image before montaging\n",
    "        #Add text with score\n",
    "        (im_width, im_height) = image.size\n",
    "        img_txt = cv2.putText(np.asarray(image),(\"UD-Score: \" + str(\"%.4f\" % sorted_embedding_results.distance.values[index])), (im_width//50, im_height//9), cv2.FONT_HERSHEY_SIMPLEX, (im_width)/320, (0, 0, 0), lineType=cv2.LINE_AA) \n",
    "        img_id = cv2.putText(img_txt,(\"PLID: \" + str(sorted_embedding_results.index.values[index])), (im_width//4, im_height//4), cv2.FONT_HERSHEY_SIMPLEX, (im_width)/300, (0, 0, 0), lineType=cv2.LINE_AA) \n",
    "    \n",
    "        #Add Border\n",
    "        bordersize = math.ceil(math.sqrt(im_width*im_width)/10)\n",
    "        img_brd=cv2.copyMakeBorder(img_id, top=bordersize, bottom=bordersize, left=bordersize, right=bordersize, \n",
    "                                   borderType= cv2.BORDER_CONSTANT, \n",
    "                                   value = (np.asarray(cmap(sorted_embedding_results.rescaled_scores.values[index]))*255))\n",
    "        \n",
    "        #Convert back to pillow format\n",
    "        image = Image.fromarray(img_brd)\n",
    "        \n",
    "        #Montage build\n",
    "        montage.paste(image, (offset_x, offset_y))\n",
    "        max_x = max(max_x, offset_x + image.size[0])\n",
    "        max_y = max(max_y, offset_y + image.size[1])\n",
    "\n",
    "        if index % row_size == row_size-1:\n",
    "            offset_y = max_y + margin\n",
    "            offset_x = 0\n",
    "        else:\n",
    "            offset_x += margin + image.size[0]\n",
    "        \n",
    "    montage = montage.crop((0, 0, max_x, max_y))\n",
    "    montage.save(output_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on: https://github.com/tensorflow/tensorflow/issues/6322\n",
    "def images_to_ranked_sprite_montage(data, output_fn):\n",
    "    \"\"\"Creates the sprite image along with any necessary padding\n",
    "    Args:\n",
    "      data: NxHxWxC tensor containing the images N=number of images and C=3 for RGB images (3-channels)\n",
    "    Returns:\n",
    "      data: Properly shaped HxWx3 image with any necessary padding.\n",
    "    \"\"\"\n",
    "    if len(data.shape) == 3:\n",
    "        data = np.tile(data[...,np.newaxis], (1,1,1,3))\n",
    "    data = data.astype(np.float32)\n",
    "    min = np.min(data.reshape((data.shape[0], -1)), axis=1)\n",
    "    data = (data.transpose(1,2,3,0) - min).transpose(3,0,1,2)\n",
    "    max = np.max(data.reshape((data.shape[0], -1)), axis=1)\n",
    "    data = (data.transpose(1,2,3,0) / max).transpose(3,0,1,2)\n",
    "â\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = ((0, n ** 2 - data.shape[0]), (0, 0), (0, 0)) + ((0, 0),) * (data.ndim - 3)\n",
    "    data = np.pad(data, padding, mode='constant',constant_values=1)\n",
    "    # Tile the individual thumbnails into an image.\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "    sprite = Image.fromarray(data)\n",
    "    sprite.save(output_fn)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display in Active window\n",
    "%matplotlib qt5\n",
    "\n",
    "#Display and Save base montage\n",
    "ranked_montage_path = 'output/analysis/Ugly_Duckling_Analysis/Output/Montages/ranked_montage_PLs.png'\n",
    "images_to_ranked_montage(sorted_embedding_results, ranked_montage_path)\n",
    "\n",
    "fig_m, ax_m = plt.subplots(num = 'Montage')\n",
    "plt.imshow(cv2.cvtColor(cv2.imread(ranked_montage_path), cv2.COLOR_BGR2RGB),cmap=plt.cm.Reds)\n",
    "plt.text(0.5, 1.05, 'Ugly Duckling Scoring Montage',\n",
    "         horizontalalignment='center',\n",
    "         fontsize=20,\n",
    "         transform = ax_m.transAxes)\n",
    "plt.axis(\"off\")\n",
    "cm_ax_m = plt.colorbar(cax = plt.axes([0.16, 0.075, 0.73, 0.025]), orientation='horizontal', ticks=[0, 255])\n",
    "cm_ax_m.ax.xaxis.set_ticks_position('bottom')\n",
    "cm_ax_m.ax.set_xticklabels(['Common','Odd'])  # vertically oriented colorbar\n",
    "\n",
    "#Save Montage with colormap\n",
    "ranked_montage_w_colormap_path = 'output/analysis/Ugly_Duckling_Analysis/Output/Montages/ranked_labeled_montage_PLs.png'\n",
    "plt.savefig(ranked_montage_w_colormap_path)\n",
    "\n",
    "\n",
    "\n",
    "# Display and Save Sprite Montage of pigmented lesions by ranking\n",
    "size = [300,300]\n",
    "cmap = cm.Reds\n",
    "img_data =[]\n",
    "i=0\n",
    "for image_num, row in sorted_embedding_results.iterrows():\n",
    "    img_path = row['image']\n",
    "    img = cv2.resize(cv2.imread(img_path),(size[0],size[1]))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_txt = cv2.putText(img,(\"UD-Score: \" + str(\"%.4f\" % sorted_embedding_results['distance'][i])), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), lineType=cv2.LINE_AA) \n",
    "    img_id = cv2.putText(img_txt,(\"PLID: \" + str(sorted_embedding_results.index.values[i])), (10,60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), lineType=cv2.LINE_AA) \n",
    "    #Add Border\n",
    "    bordersize = 30\n",
    "    img_brd=cv2.copyMakeBorder(img_id, top=bordersize, bottom=bordersize, left=bordersize, right=bordersize, \n",
    "                                   borderType= cv2.BORDER_CONSTANT, \n",
    "                                   value = (np.asarray(cmap(sorted_embedding_results.rescaled_scores.values[i]))*255))\n",
    "    img_rsh = np.reshape(img_brd, (size[0]+2*bordersize,size[1]+2*bordersize,3,1))\n",
    "    img_data.append(img_rsh)\n",
    "    i+=1\n",
    "    \n",
    "img_data = np.squeeze(np.stack(img_data,axis=0))\n",
    "ranked_sprite_path = 'output/analysis/Ugly_Duckling_Analysis/Output/Montages/ranked_labeled_sprite_PLs.png'\n",
    "ranked_sprite_montage = images_to_ranked_sprite_montage(img_data,ranked_sprite_path)\n",
    "\n",
    "#Plot\n",
    "fig_s, ax_s = plt.subplots(num = 'Sprite')\n",
    "plt.imshow(ranked_sprite_montage,cmap=plt.cm.Reds)\n",
    "plt.text(0.5, 1.05, 'Ugly Duckling Scoring Sprite',\n",
    "         horizontalalignment='center',\n",
    "         fontsize=20,\n",
    "         transform = ax_s.transAxes)\n",
    "plt.axis(\"off\")\n",
    "cm_ax_s = plt.colorbar(cax = plt.axes([0.21, 0.075, 0.6, 0.025]), orientation='horizontal', ticks=[0, 255])\n",
    "cm_ax_s.ax.xaxis.set_ticks_position('bottom')\n",
    "cm_ax_s.ax.set_xticklabels(['Common','Odd'])  # vertically oriented colorbar\n",
    "\n",
    "#Save Montage with colormap\n",
    "ranked_montage_sprite_w_colormap_path = 'output/analysis/Ugly_Duckling_Analysis/Output/Montages/ranked_labeled_sprite_montage_PLs.png'\n",
    "plt.savefig(ranked_montage_sprite_w_colormap_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pigmented Lesion Images t-SNE\n",
    "\n",
    "This notebook will take you through the process of generating a [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) of a set of images, using a feature vector for each image derived from the activations of the last fully-connected layer in a pre-trained convolutional neural network (convnet). Based on code by \"ml4a\" https://github.com/ml4a/ml4a-guides/blob/master/notebooks/image-tsne.ipynb\n",
    "\n",
    "### Prepare intra-patient Pigmented lesion dataset\n",
    "\n",
    "Finally, prepare a folder of images per patient to do the analysis on. Each folder will contain roughly 100 images. Run the following commands inside a folder of your choosing (this notebook will assume you do so in the `output/analysis/Ugly_Duckling_Analysis/Pigmented_Lesions/` folder.\n",
    "\n",
    "First, we will load our image paths and feature vectors from the previous notebook into memory. We can print their contents to get an idea of what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Paths to images and features from pickle file\n",
    "images, features, ROI_PLs  = pickle.load(open('output/analysis/Ugly_Duckling_Analysis/Output/features_IntraPatient_PLs.p', 'rb'))\n",
    "\n",
    "#Visualize example of features\n",
    "for i, f in list(zip(images, features))[0:5]:\n",
    "    print(\"image: %s, features: %0.2f,%0.2f,%0.2f,%0.2f... \"%(i, f[0], f[1], f[2], f[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is usually a good idea to first run the vectors through a faster dimensionality reduction technique like [principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) to project your data into an intermediate lower-dimensional space before using t-SNE. This improves accuracy, and cuts down on runtime since PCA is more efficient than t-SNE. Since we have already projected our data down with PCA in the previous notebook, we can proceed straight to running the t-SNE on the feature vectors. Run the command in the following cell, taking note of the arguments:\n",
    "\n",
    "- `n_components` is the number of dimensions to project down to. In principle it can be anything, but in practice t-SNE is almost always used to project to 2 or 3 dimensions for visualization purposes.\n",
    "- `learning_rate` is the step size for iterations. You usually won't need to adjust this much, but your results may vary slightly. \n",
    "- `perplexity` refers to the number of independent clusters or zones t-SNE will attempt to fit points around. Again, it is relatively robust to large changes, and usually 20-50 works best. \n",
    "- `angle` controls the speed vs accuracy tradeoff. Lower angle means better accuracy but slower, although in practice, there is usually little improvement below a certain threshold.\n",
    "- `n_components` : int, optional (default: 2). Dimension of the embedded space.\n",
    "- `perplexity ` : float, optional (default: 30). The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. The choice is not extremely critical since t-SNE is quite insensitive to this parameter.\n",
    "- `early_exaggeration `: float, optional (default: 12.0). Controls how tight natural clusters in the original space are in the embedded space and how much space will be between them. For larger values, the space between natural clusters will be larger in the embedded space. Again, the choice of this parameter is not very critical. If the cost function increases during initial optimization, the early exaggeration factor or the learning rate might be too high.\n",
    "- `learning_rate `: float, optional (default: 200.0). The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If the learning rate is too high, the data may look like a \"ball\" with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help.\n",
    "- `n_iter `: int, optional (default: 1000). Maximum number of iterations for the optimization. Should be at least 250.\n",
    "- `n_iter_without_progress `: int, optional (default: 300). Maximum number of iterations without progress before we abort the optimization, used after 250 initial iterations with early exaggeration. Note that progress is only checked every 50 iterations so this value is rounded to the next multiple of 50. New in version 0.17: parameter n_iter_without_progress to control stopping criteria.\n",
    "- `min_grad_norm `: float, optional (default: 1e-7). If the gradient norm is below this threshold, the optimization will be stopped.\n",
    "- `metric `: string or callable, optional. The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is \"precomputed\", X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. The default is \"euclidean\" which is interpreted as squared euclidean distance.\n",
    "- `init `: string or numpy array, optional (default: \"random\"). Initialization of embedding. Possible options are \"random\", \"pca\", and a numpy array of shape (n_samples, n_components). PCA initialization cannot be used with precomputed distances and is usually more globally stable than random initialization.\n",
    "- `verbose `: int, optional (default: 0). Verbosity level.\n",
    "- `random_state `: int, RandomState instance or None, optional (default: None). If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Note that different initializations might result in different local minima of the cost function.\n",
    "- `method `: string (default: 'barnes_hut'). By default the gradient calculation algorithm uses Barnes-Hut approximation running in O(NlogN) time. method=\"exact\" will run on the slower, but exact, algorithm in O(N^2) time. The exact algorithm should be used when nearest-neighbor errors need to be better than 3%. However, the exact method cannot scale to millions of examples. New in version 0.17: Approximate optimization method via the Barnes-Hut.\n",
    "- `angle `: float (default: 0.5). Only used if method=\"barnes_hut\" This is the trade-off between speed and accuracy for Barnes-Hut T-SNE. \"angle\" is the angular size (referred to as theta in [3]) of a distant node as measured from a point. If this size is below \"angle\" then it is used as a summary node of all points contained within it. This method is not very sensitive to changes in this parameter in the range of 0.2 - 0.8. Angle less than 0.2 has quickly increasing computation time and angle greater 0.8 has quickly increasing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(pca_features)\n",
    "tsne = TSNE(n_components = 3,\n",
    "            perplexity = 5, \n",
    "            early_exaggeration = 12.0,\n",
    "            learning_rate = 0.1,\n",
    "            n_iter = 5000,\n",
    "            n_iter_without_progress = 300,\n",
    "            min_grad_norm = 1e-7,\n",
    "            metric = 'l2',\n",
    "            init = 'pca',\n",
    "            verbose = 2,\n",
    "            random_state = 1,\n",
    "            method = 'exact',\n",
    "            angle = 0.5\n",
    "           ).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, t-SNE uses an iterative approach, making small (or sometimes large) adjustments to the points. By default, t-SNE will go a maximum of 1000 iterations, but in practice, it often terminates early because it has found a locally optimal (good enough) embedding.\n",
    "\n",
    "The variable `tsne` contains an array of unnormalized 2d points, corresponding to the embedding. In the next cell, we normalize the embedding so that lies entirely in the range (0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx, ty, tz = tsne[:,0], tsne[:,1], tsne[:,2]\n",
    "tx = (tx-np.min(tx)) / (np.max(tx) - np.min(tx))\n",
    "ty = (ty-np.min(ty)) / (np.max(ty) - np.min(ty))\n",
    "tz = (tz-np.min(tz)) / (np.max(tz) - np.min(tz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will compose a new RGB image where the set of images have been drawn according to the t-SNE results. Adjust `width` and `height` to set the size in pixels of the full image, and set `max_dim` to the pixel size (on the largest size) to scale images to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define size of t-SNE display images\n",
    "width = 2500\n",
    "height = 2000\n",
    "max_dim = 100\n",
    "\n",
    "#normal charts\n",
    "full_image_xy = Image.new('RGBA', (width, height))\n",
    "for img, x, y in tqdm(zip(images, tx, ty)):\n",
    "    tile = Image.open(img)\n",
    "    rs = max(1, tile.width/max_dim, tile.height/max_dim)\n",
    "    tile = tile.resize((int(tile.width/rs), int(tile.height/rs)), Image.ANTIALIAS)\n",
    "    full_image_xy.paste(tile, (int((width-max_dim)*x), int((height-max_dim)*y)), mask=tile.convert('RGBA'))\n",
    "#Save image to disk\n",
    "full_image_xy.save('output/analysis/Ugly_Duckling_Analysis/Output/t-sne/PLs-tSNE-Analysis_XY.png')\n",
    "\n",
    "full_image_xz = Image.new('RGBA', (width, height))\n",
    "for img, x, z in tqdm(zip(images, tx, tz)):\n",
    "    tile = Image.open(img)\n",
    "    rs = max(1, tile.width/max_dim, tile.height/max_dim)\n",
    "    tile = tile.resize((int(tile.width/rs), int(tile.height/rs)), Image.ANTIALIAS)\n",
    "    full_image_xz.paste(tile, (int((width-max_dim)*x), int((height-max_dim)*z)), mask=tile.convert('RGBA'))\n",
    "#Save image to disk\n",
    "full_image_xz.save('output/analysis/Ugly_Duckling_Analysis/Output/t-sne/PLs-tSNE-Analysis_XZ.png')\n",
    "\n",
    "\n",
    "full_image_yz = Image.new('RGBA', (width, height))\n",
    "for img, y, z in tqdm(zip(images, ty, tz)):\n",
    "    tile = Image.open(img)\n",
    "    rs = max(1, tile.width/max_dim, tile.height/max_dim)\n",
    "    tile = tile.resize((int(tile.width/rs), int(tile.height/rs)), Image.ANTIALIAS)\n",
    "    full_image_yz.paste(tile, (int((width-max_dim)*y), int((height-max_dim)*z)), mask=tile.convert('RGBA'))\n",
    "#Save image to disk\n",
    "full_image_yz.save('output/analysis/Ugly_Duckling_Analysis/Output/t-sne/PLs-tSNE-Analysis_YZ.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot t-SNE\n",
    "# Display in Active window\n",
    "\n",
    "xs = tsne[:,0]\n",
    "ys = tsne[:,1]\n",
    "zs = tsne[:,2]\n",
    "\n",
    "#Create base plot\n",
    "fig_tn = plt.figure(num = 't-SNE')\n",
    "\n",
    "#Create 3D scater plot\n",
    "ax_tn = fig_tn.add_subplot(111, projection=Axes3D.name)\n",
    "ax_tn.scatter(xs,ys,zs, marker=\"o\", c='w')\n",
    "\n",
    "# Create a dummy axes to place annotations to\n",
    "ax_tn_i = fig_tn.add_subplot(111,frame_on=False) \n",
    "ax_tn_i.axis(\"off\")\n",
    "\n",
    "#Init t-sne zoom\n",
    "im_zoom=0.1\n",
    "\n",
    "class ImageAnnotations3D():\n",
    "    def __init__(self, xyz, imgs, ax3d, ax2d, im_zoom):\n",
    "        self.xyz = xyz\n",
    "        self.imgs = imgs\n",
    "        self.ax3d = ax3d\n",
    "        self.ax2d = ax2d\n",
    "        self.annot = []\n",
    "        self.im_zoom = im_zoom\n",
    "        for s,im in zip(self.xyz, self.imgs):\n",
    "            x,y = self.proj(s)\n",
    "            self.annot.append(self.image(im,[x,y],self.im_zoom))\n",
    "                \n",
    "        self.lim = self.ax3d.get_w_lims()\n",
    "        self.rot = self.ax3d.get_proj()\n",
    "        self.cid = self.ax3d.figure.canvas.mpl_connect(\"draw_event\",self.update)\n",
    "\n",
    "    def proj(self, X):\n",
    "        \"\"\" From a 3D point in axes ax1, \n",
    "            calculate position in 2D in ax2 \"\"\"\n",
    "        x,y,z = X\n",
    "        x2, y2, _ = proj3d.proj_transform(x,y,z, self.ax3d.get_proj())\n",
    "        tr = self.ax3d.transData.transform((x2, y2))\n",
    "        return self.ax2d.transData.inverted().transform(tr)\n",
    "\n",
    "    def image(self,arr,xy,im_zoom):\n",
    "        \"\"\" Place an image (arr) as annotation at position xy \"\"\"\n",
    "        im = offsetbox.OffsetImage(arr, zoom=im_zoom)\n",
    "        im.image.axes = ax_tn\n",
    "        ab = offsetbox.AnnotationBbox(im, xy, xybox=(0., 0.),\n",
    "                            xycoords='data', boxcoords=\"offset points\",\n",
    "                            pad=0.0)\n",
    "        self.ax2d.add_artist(ab)\n",
    "        return ab\n",
    "\n",
    "    def update(self,event):\n",
    "        if np.any(self.ax3d.get_w_lims() != self.lim) or \\\n",
    "                        np.any(self.ax3d.get_proj() != self.rot):\n",
    "            self.lim = self.ax3d.get_w_lims()\n",
    "            self.rot = self.ax3d.get_proj()\n",
    "            for s,ab,im in zip(self.xyz, self.annot, self.imgs):\n",
    "                ab.xy = self.proj(s)\n",
    "                ab.offsetbox.set_zoom(im_zoom*(szoom.val/100))\n",
    "\n",
    "                \n",
    "imgs = [cv2.cvtColor(cv2.imread(images[i]), cv2.COLOR_BGR2RGB) for i in range(len(images))]\n",
    "imgs_brd = imgs.copy()\n",
    "for i in range(len(imgs)):\n",
    "    (im_width, im_height, im_depth) = imgs[i].shape\n",
    "    bordersize = math.ceil(math.sqrt(im_width*im_width)/10)\n",
    "    imgs_brd[i]=cv2.copyMakeBorder(imgs[i], top=bordersize, bottom=bordersize, left=bordersize, right=bordersize, \n",
    "                                 borderType= cv2.BORDER_CONSTANT, \n",
    "                                 value = (np.asarray(cmap(embedding_results.rescaled_scores.values[i]))*255))\n",
    "\n",
    "ia = ImageAnnotations3D(np.c_[xs,ys,zs], imgs_brd, ax_tn, ax_tn_i, im_zoom)\n",
    "ax_range = ax_tn.get_w_lims()\n",
    "\n",
    "#ax.set_title('Normalized 3D t-SNE')\n",
    "ax_tn.set_xlabel('X Label')\n",
    "ax_tn.set_ylabel('Y Label')\n",
    "ax_tn.set_zlabel('Z Label')\n",
    "\n",
    "# Get rid of colored axes planes\n",
    "# First remove fill\n",
    "ax_tn.xaxis.pane.fill = False\n",
    "ax_tn.yaxis.pane.fill = False\n",
    "ax_tn.zaxis.pane.fill = False\n",
    "\n",
    "# Now set color to white (or whatever is \"invisible\")\n",
    "ax_tn.xaxis.pane.set_edgecolor('k')\n",
    "ax_tn.yaxis.pane.set_edgecolor('k')\n",
    "ax_tn.zaxis.pane.set_edgecolor('k')\n",
    "\n",
    "# Get rid of the ticks                          \n",
    "ax_tn.set_xticks([])                               \n",
    "ax_tn.set_yticks([])                               \n",
    "ax_tn.set_zticks([])\n",
    "\n",
    "# Add the labels\n",
    "ax_tn.set_xlabel('X(s)')\n",
    "ax_tn.set_ylabel('Y(s)')\n",
    "ax_tn.set_zlabel('Z(s)')\n",
    "\n",
    "# Get rid of the grid as well:\n",
    "ax_tn.grid(False)\n",
    "\n",
    "# Define Sliders for Elevation (elev) and Azimutal (azim)\n",
    "e0 = 30  # Initial State (isometric)\n",
    "a0 = -60  # Initial State (isometric)\n",
    "z0 = 100  # Initial State (isometric)\n",
    "\n",
    "axcolor = 'lightgoldenrodyellow'\n",
    "axelev = plt.axes([0.09, 0.96, 0.3, 0.03], facecolor=axcolor)\n",
    "axazim = plt.axes([0.09, 0.92, 0.3, 0.03], facecolor=axcolor)\n",
    "axzoom = plt.axes([0.09, 0.88, 0.3, 0.03], facecolor=axcolor)\n",
    "\n",
    "selev = Slider(axelev, 'X-Z', -180.0, 180.0, valinit=e0)\n",
    "sazim = Slider(axazim, 'X-Y', -180.0, 180.0, valinit=a0)\n",
    "szoom = Slider(axzoom, 'Zoom%', 1.0, 500.0, valinit=z0)\n",
    "\n",
    "\n",
    "def update(val):\n",
    "    azim = sazim.val\n",
    "    elev = selev.val\n",
    "    zoom = szoom.val\n",
    "    ax_tn.view_init(elev, azim)\n",
    "    adj_zoom = (100.0 / zoom)\n",
    "    ax_tn.set_xlim3d(ax_range[0]*adj_zoom,ax_range[1]*adj_zoom)\n",
    "    ax_tn.set_ylim3d(ax_range[2]*adj_zoom,ax_range[3]*adj_zoom)\n",
    "    ax_tn.set_zlim3d(ax_range[4]*adj_zoom,ax_range[5]*adj_zoom)\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "def reset(event):\n",
    "    selev.reset()\n",
    "    sazim.reset()\n",
    "    szoom.reset()\n",
    "\n",
    "# Exeute Handlers\n",
    "selev.on_changed(update)\n",
    "sazim.on_changed(update)\n",
    "szoom.on_changed(update)\n",
    "resetax = plt.axes([0.85, 0.05, 0.1, 0.04])\n",
    "button = Button(resetax, 'Reset', color=axcolor, hovercolor='0.975')\n",
    "button.on_clicked(reset)\n",
    "plt.show()\n",
    "\n",
    "#Save t-sne\n",
    "ranked_ugly_ducking_path = 'output/analysis/Ugly_Duckling_Analysis/Output/t-sne/CNN_ugly_duckling_t-sne.png'\n",
    "plt.savefig(ranked_ugly_ducking_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intra-patient Ugly Ducking Analysis (CNN-feature based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram\n",
    "#Calculate feature embedding (FE) distance histogram to help detect number of top ourliers\n",
    "fe_distances = sorted_embedding_results['distance'].astype(float)\n",
    "plt.figure(num = 'UD-Histogram', figsize=(8,4))\n",
    "plt.title ('Pigmented lesion Oddness Score frequency(Outliers are low-freq with high value)')\n",
    "plt.xlabel('Geometrical Distance (Cosine)')\n",
    "fig_hist = fe_distances.plot.hist(bins = 30) #20 because it is very unlikely that a human has more than 20 different classes of pigmented lesions\n",
    "\n",
    "# Get top_percent values to then suggest background (bg) outliers\n",
    "outliers = int(len(fe_distances) * top_percent_thresh)\n",
    "top_outliers = fe_distances.nlargest(n=outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define wide-field feature embedding PL outlier analysis (Ugly Duckling)\n",
    "def wide_field_feature_embedding_saliency_analysis(wf_orig_image, ROI_PLs, embedding_results):\n",
    "    ## SALIENCY CODE\n",
    "    # Inputs: \n",
    "    #    \"wf_orig_image\" is the original RGB image\n",
    "    \n",
    "    fes_img = wf_montage_BW_image.copy()*0\n",
    "    cmap=plt.cm.jet\n",
    "    #Iterate over pigmented lesions and paint over color given the cosine distance from the cnn features\n",
    "    for index, row in embedding_results.iterrows():\n",
    "        (x,y,r) = np.uint(ROI_PLs[index])\n",
    "        color = (np.asarray(cmap(embedding_results.rescaled_scores.values[index]))*255)\n",
    "        fes_img = cv2.circle(fes_img,(x,y), r, color, -1)\n",
    "        #fes_img = cv2.cvtColor(fes_img, cv2.COLOR_BGR2RGB)\n",
    "        (im_width, im_height) = fes_img.shape[:2]\n",
    "        r = 100.0 / im_height\n",
    "        dim = (100, int(im_width * r))\n",
    "        res_fes_img = cv2.resize(fes_img, dim, interpolation = cv2.INTER_AREA)\n",
    "        res_fes_img = cv2.GaussianBlur(res_fes_img,(5,5),0)\n",
    "        feature_embedding_saliency_img = cv2.resize(res_fes_img, (im_height, im_width), interpolation = cv2.INTER_CUBIC)\n",
    "        # Merge Wide field image with heatmap\n",
    "        wf_feature_embedding_overlay_montage_RGB_image = cv2.addWeighted(wf_orig_image, 0.75, feature_embedding_saliency_img, 0.75, 0)\n",
    "    return wf_feature_embedding_overlay_montage_RGB_image, feature_embedding_saliency_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process CNN based ugly duckling image\n",
    "wf_feature_embedding_overlay_montage_RGB_image, feature_embedding_saliency_img = wide_field_feature_embedding_saliency_analysis(wf_orig_image, ROI_PLs, embedding_results)\n",
    "\n",
    "#CLAHE\n",
    "wf_feature_embedding_overlay_montage_RGB_image = apply_clahe(wf_feature_embedding_overlay_montage_RGB_image)\n",
    "\n",
    "#Display and Save base montage\n",
    "fig_ug, ax_ug = plt.subplots(num = 'CNN Ugly Ducking Heatmap')\n",
    "plt.imshow(wf_feature_embedding_overlay_montage_RGB_image,cmap=plt.cm.jet)\n",
    "#plt.imshow(cv2.cvtColor(wf_feature_embedding_overlay_montage_RGB_image, cv2.COLOR_BGR2RGB),cmap=plt.cm.jet)\n",
    "#plt.text(0.5, 1.05, 'Ugly Duckling Heatmap',\n",
    "#         horizontalalignment='center',\n",
    "#         fontsize=20,\n",
    "#         transform = ax_m.transAxes)\n",
    "plt.title('Ugly Duckling Heatmap')\n",
    "plt.axis(\"off\")\n",
    "cm_ax_m = plt.colorbar(cax = plt.axes([0.10, 0.075, 0.8, 0.025]), orientation='horizontal', ticks=[0, 255])\n",
    "cm_ax_m.ax.xaxis.set_ticks_position('bottom')\n",
    "cm_ax_m.ax.set_xticklabels(['Common','Odd'])  # vertically oriented colorbar\n",
    "\n",
    "#Save with colormap\n",
    "ranked_ugly_ducking_path = 'output/analysis/CNN_ugly_duckling_img.png'\n",
    "\n",
    "#Save with colorbar\n",
    "plt.savefig(ranked_ugly_ducking_path)\n",
    "cv2.imwrite(ranked_ugly_ducking_path, wf_feature_embedding_overlay_montage_RGB_image)\n",
    "\n",
    "#Save without colorbar\n",
    "#imageio.imwrite(ranked_ugly_ducking_path, wf_feature_embedding_overlay_montage_RGB_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save csv on lesion ranking\n",
    "numbers = [x.strip('output/analysis/Ugly_Duckling_Analysis/Pigmented_Lesions/P_') for x in embedding_results.image]\n",
    "numbers = [x.strip('.') for x in numbers]\n",
    "\n",
    "a=np.stack((numbers,embedding_results.rescaled_scores), axis=-1)\n",
    "b = [[i, int(a[list(a[:,0]).index(str(i)),1]) if ((str(i) in list(a[:,0]))) else 0] for i in range(300)]\n",
    "c = np.matrix(b).transpose()\n",
    "np.savetxt(\"output/analysis/Ugly_Duckling_Analysis/UD_Scores.csv\", c, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Session outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Folder SAVING ON DATED BACKUP FOLDER\n",
    "# Save the entire current model folder to a backup folder\n",
    "img_path ='data/wide_field_database/UglyDucklingTest/Wide_Field_Original/' + img_name + '.tiff'\n",
    "source_model_path = 'output/analysis/'\n",
    "backup_model_path = 'output/backup/' + datetime.now().strftime('%Y%m%d') + '/' + img_name + '/' + source_model_path \n",
    "## Create folder to store model (if not existent)\n",
    "if not os.path.isdir(backup_model_path):\n",
    "    os.makedirs(backup_model_path)\n",
    "\n",
    "# Copy all contents to dated backup\n",
    "copy_full_dir(source_model_path, backup_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for user click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close all windows after click\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------\n",
    "END OF CODE"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
